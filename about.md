---
layout: page
title: About
permalink: /about/
---

I'm Alexis Huet. I got a PhD of mathematics in 2014 from the University of Lyon (France). I'm currently working in data science.

On this blog, I provide detailed posts about specific topics, 

### About deep learning

- [RNN with Keras: Predicting time series](https://ahstat.github.io/RNN-Keras-time-series/)<img src="images/Python_logo.png", width="20", height="20"/>: Complete introduction of time series prediction with RNN. This tutorial has been written for answering a [stackoverflow post](https://stackoverflow.com/questions/41947039/keras-rnn-with-lstm-cells-for-predicting-multiple-output-time-series-based-on-mu/48521460#48521460), and has been used later [in a real-world context](https://stackoverflow.com/questions/48929272/non-linear-multivariate-time-series-response-prediction-using-rnn/49666510#49666510).

- [RNN with Keras: Understanding computations](https://ahstat.github.io/RNN-Keras-understanding-computations/)<img src="images/Python_logo.png", width="20", height="20"/>: Highlights structure of common RNN algorithms by following computations carried out by each model. It provides a clear summary of command lines, math equations and diagrams.

### About maths in data science

- [Optimizing GMM parameters using EM](https://ahstat.github.io/Optimizing-GMM-using-EM/) Description of GMM; How to update parameters using EM; Illustration on a simple example. Unlike many other sources, I fully detail parameters' update using gradient and Hessian.

- [Rediscover EM algorithm from scratch](https://ahstat.github.io/Rediscover-EM-algorithm/) Many introductions of EM exist on the web. This one starts from the likelihood computation problem and uses inductive reasoning to bring out EM.

- [Computation of the gradient for SNE](https://ahstat.github.io/Gradient-for-SNE/) Deriving gradient of the SNE algorithm, fully detailed.

-[An illustration of Metropolisâ€“Hastings algorithm](https://ahstat.github.io/Metropolis-Hastings-example/): todo

- [Maximizing likelihood is equivalent to minimizing KL-divergence](https://ahstat.github.io/Kullback-Leibler-divergence/) Restating this classic equivalence in my "own" words.

### Own projects

- [Langton's ant extended to Voronoi tessellations](https://ahstat.github.io/Langton-Voronoi/)(https://ahstat.github.io/Coal/) ![](../images/R_logo.png =20x): A program extending Langton's ant to any Voronoi tessellation of the plane. Simulations show interesting walks for some partitions of the plane, including chaotic structures, highway patterns and even bounded evolutions.

- [Coal: Composition of Linear Functions](https://ahstat.github.io/Coal/) ![](./images/R_logo.png =20x): A program for automating composition of linear functions. `gmp` package has been used to keep exact results for big rational numbers.

- [Triangle pursuit](https://ahstat.github.io/Triangle-pursuit/) ![](images/R_logo.png =20x): A program computing recurrent sequences, offering generalization for different rules, different norms, larger number of initial points and higher dimensions.





Answers:
- [Keras RNN with LSTM cells for predicting multiple output time series based on multiple intput time series](https://stackoverflow.com/questions/41947039/keras-rnn-with-lstm-cells-for-predicting-multiple-output-time-series-based-on-mu/48521460#48521460)
- [Good accuracy despite high loss value](https://stats.stackexchange.com/questions/258166/good-accuracy-despite-high-loss-value/281651#281651)

Questions:
- [Clustering as dimensionality reduction](https://stats.stackexchange.com/questions/288668/clustering-as-dimensionality-reduction)
- [Remove anti-aliasing for pandas plot.area](https://stackoverflow.com/questions/44612797/remove-anti-aliasing-for-pandas-plot-area)




### Contact me

ahstat@qq.com