---
layout: post
title: RNN with Keras&#58; Understanding computations
published: true
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

This tutorial highlights structure of common RNN algorithms by following and understanding computations carried out by each model.
It is intended for anyone without prior understanding of RNN.
If you really never heard about RNN, you *need* to [read this post of Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) first.

The present post focuses on understanding step by step computations in each model, without paying attention to train something useful.
It is illustrated with [Keras](https://keras.io/) codes
and divided in five parts:

- TimeDistributed component,
- Simple RNN,
- Simple RNN with two hidden layers,
- LSTM,
- GRU.


Inputs and outputs for this section
Model definition and training
Understanding the weights
Understanding the computations






## Part A: Explanation of TimeDistributed component


### Inputs and outputs for this section



### Model definition and training

### Prediction of new inputs

### Understanding the computations


### Explanation of TimeDistributed with more dimensions



## Part B: Explanation of simple RNN



### Inputs and outputs for this section



### Model definition and training


### Understanding the weights


### Understanding the computations



## Part C: Explanation of simple RNN with two hidden layers



### Inputs and outputs for this section



### Model definition and training


### Understanding the weights


### Understanding the computations


## Part D: Explanation of LSTM


### Inputs and outputs for this section



### Model definition and training


### Understanding the weights


### Understanding the computations


## Part E: Explanation of GRU


### Inputs and outputs for this section



### Model definition and training


### Understanding the weights


### Understanding the computations

